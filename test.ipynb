{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma database does not exist. Creating a new one...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 7/7 [00:14<00:00,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "local_path = \"case.pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if local_path:\n",
    "    loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "    data = loader.load()\n",
    "else:\n",
    "    print(\"Upload a PDF file\")\n",
    "    data = None  # Ensure data is defined\n",
    "    exit()  # Exit if no data is loaded\n",
    "\n",
    "# Split the text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_of_case\")\n",
    "\n",
    "# Check if the Chroma database already exists\n",
    "if os.path.exists(persistent_directory):\n",
    "    print(\"Chroma database already exists. Loading existing database...\")\n",
    "    vector_db = Chroma(persist_directory=persistent_directory, embedding_function=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "else:\n",
    "    print(\"Chroma database does not exist. Creating a new one...\")\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks, \n",
    "        embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "        collection_name=\"local-rag\",\n",
    "        persist_directory=persistent_directory\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# LLM from Ollama\n",
    "local_model = \"llama3\"\n",
    "llm = ChatOllama(model=local_model)\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(search_kwargs={\"k\": 3}), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, it appears that the authors of this paper (Attention Is All You Need) are presenting a new approach to natural language processing (NLP) called the \"Transformer\" model. Specifically, they are describing a sequence-to-sequence transduction model that replaces traditional recurrent neural network (RNN) layers with multi-headed self-attention mechanisms.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        \n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = chain.invoke({\"question\": query})\n",
    "        \n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result['result']}\")\n",
    "        \n",
    "        # Update the chat history\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": result['result']})\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the AI! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:04<00:00,  4.67s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 75.95 seconds\n",
      "AI: Based on the given context, I would answer that this is about a paper titled \"Attention Is All You Need\" published in NIPS 2017, which presents a novel sequence-to-sequence model called the Transformer that relies entirely on self-attention mechanisms and achieves state-of-the-art results on several machine translation tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:05<00:00,  5.29s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time: 142.11 seconds\n",
      "AI: Based on the provided context, I will attempt to explain the concept of \"Scaled Dot-Product Attention\".\n",
      "\n",
      "In the Transformer model, Scaled Dot-Product Attention is a mechanism used in both the encoder and decoder layers. It allows each position in the input sequence (or output sequence) to attend to all positions in the same sequence.\n",
      "\n",
      "The attention process involves computing dot products between query vectors and key vectors, dividing by the square root of the key vector's dimension, and then applying a softmax function to obtain the weights on the values. This is illustrated in Figure 2 (left).\n",
      "\n",
      "In other words, Scaled Dot-Product Attention allows each position in the input sequence to consider all positions in that sequence as potential references when generating its output. This mechanism mimics human-like attention patterns, where we tend to focus on certain parts of a sentence or text more than others.\n",
      "\n",
      "The Transformer model uses this mechanism in two ways: 1) \"encoder-decoder attention\" layers, where the queries come from previous decoder layers and the memory keys and values come from the output of the encoder; and 2) self-attention layers within the encoder, where all positions in the input sequence can attend to each other.\n",
      "\n",
      "This concept is essential for the Transformer model's success in tasks like machine translation and natural language processing.\n"
     ]
    }
   ],
   "source": [
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        \n",
    "        import time\n",
    "        start_time = time.time()  # Start time for profiling\n",
    "        \n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = chain.invoke({\"question\": query})\n",
    "        \n",
    "        end_time = time.time()  # End time for profiling\n",
    "        print(f\"Processing time: {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result}\")\n",
    "        \n",
    "        # Update the chat history\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": result})\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma database already exists. Loading existing database...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Define paths\n",
    "case_file_path = \"case.pdf\"  # Path to the case file\n",
    "\n",
    "# Load and process the case file\n",
    "if case_file_path:\n",
    "    loader = UnstructuredPDFLoader(case_file_path)\n",
    "    data = loader.load()\n",
    "else:\n",
    "    print(\"Upload a case file\")\n",
    "    data = None\n",
    "    exit()\n",
    "\n",
    "# Split the text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_for_case2\")\n",
    "\n",
    "# Check if the Chroma database already exists\n",
    "if os.path.exists(persistent_directory):\n",
    "    print(\"Chroma database already exists. Loading existing database...\")\n",
    "    vector_db = Chroma(persist_directory=persistent_directory, embedding_function=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "else:\n",
    "    print(\"Chroma database does not exist. Creating a new one...\")\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks, \n",
    "        embedding=OllamaEmbeddings(model=\"nomic-embed-text\", show_progress=True),\n",
    "        collection_name=\"local-rag\",\n",
    "        persist_directory=persistent_directory\n",
    "    )\n",
    "\n",
    "# LLM from Ollama\n",
    "local_model = \"llama3\"\n",
    "llm = ChatOllama(model=local_model)\n",
    "\n",
    "# Query prompt for generating multiple questions\n",
    "LEGAL_QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are a legal assistant helping clients represent themselves in legal matters. Given the user's question about their case, provide clear, actionable advice based on the provided case details. Ensure your response aligns with the legal information available and is understandable for someone without legal expertise.\n",
    "    Original question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# MultiQueryRetriever setup\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=LEGAL_QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt template\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Define the chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Function to simulate a continual chat\n",
    "# def continual_chat():\n",
    "#     print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "#     chat_history = []\n",
    "#     while True:\n",
    "#         query = input(\"You: \")\n",
    "#         if query.lower() == \"exit\":\n",
    "#             break\n",
    "        \n",
    "        \n",
    "#         try:\n",
    "#             result = chain.invoke({\"question\": query})\n",
    "#             answer = result['result']\n",
    "#         except Exception as e:\n",
    "#             answer = f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "#         print(f\"AI: {answer}\")\n",
    "        \n",
    "#         chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "#         chat_history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "# # Main function to start the continual chat\n",
    "# if __name__ == \"__main__\":\n",
    "#     continual_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"what are the ways in which i can defend myself in court\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
